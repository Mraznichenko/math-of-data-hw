{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1\n",
        "\n",
        "#### EE-556 Mathematics of Data - Fall 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we consider a multiclass classification task modeled by multinomial (softmax) logistic regression. Your goal will be to analyze the estimator and its properties (convexity, existence/uniqueness), and to derive gradients/Hessians and smoothness bounds. The first part consists of theoretical questions only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "  ‚ÑπÔ∏è <strong>Information on group based work:</strong>\n",
        "</div>\n",
        "\n",
        "- You are to deliver only 1 notebook per group.\n",
        "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
        "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know.\n",
        "- Only one member of the group is allowed to use AI. We will require sharing the conversation history with the AI in the form of a public link. If you use multiple conversations across the same or multiple tools please share all of them. Name the person in your group who is allowed to use AI. We encourage you to use the AI to help you understand the material, but we ask you to write the code and theory solutions by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
        "  ‚ö†Ô∏è Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Person 1 **Luis Falke**: || Person 1 **414954**:\n",
        "\n",
        "\n",
        "Person 2 **Name**: || Person 2 **SCIPER**:\n",
        "\n",
        "\n",
        "Person 3 **Name**: || Person 3 **SCIPER**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #0a0; background-color: #dfd; padding: 10px; border-radius: 5px;\">\n",
        "  üìì Feedback on AI use: Please use the following cell to provide feedback on the AI use in this notebook.\n",
        "  \n",
        "  For example, how useful were the tools to you? Which tools did you use? Did you feel like they helped you understand the material better?\n",
        "</div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Multiclass Softmax Logistic Regression - 15 Points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now model multiclass classification with classes $c \\in \\{1,\\dots,C\\}$. For each sample $(\\mathbf{a}_i, b_i)$ with $\\mathbf{a}_i \\in \\mathbb{R}^p$ and $b_i \\in \\{1,\\dots,C\\}$, let $\\mathbf{X} = [\\mathbf{x}_1,\\dots,\\mathbf{x}_C] \\in \\mathbb{R}^{p\\times C}$ be the class weight matrix. The softmax model defines\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(b_i = c \\mid \\mathbf{a}_i) = \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_c)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}.\n",
        "$$\n",
        "\n",
        "Assume i.i.d. samples $\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n$. Our goal is to estimate $\\mathbf{X}$ by maximum likelihood (and later with an $\\ell_2$ regularizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(a)__ (1 point) Show that the negative log-likelihood $f$ can be written as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        " f(\\mathbf{X})\n",
        " &= - \\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
        " &= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "For one sample $(\\mathbf{a}_i, b_i)$, the log-probability of the observed class $b_i$ is\n",
        "\n",
        "$$\n",
        "\\log \\mathbb{P}(b_i \\mid \\mathbf{a}_i)\n",
        "= \\log \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_{b_i})}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}\n",
        "= \\mathbf{a}_i^\\top \\mathbf{x}_{b_i} - \\log\\left(\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)\\right).\n",
        "$$\n",
        "\n",
        "Assuming i.i.d. samples, the likelihood is the product\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(b_1,\\dots,b_n \\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n, \\mathbf{X})\n",
        "= \\prod_{i=1}^n \\mathbb{P}(b_i \\mid \\mathbf{a}_i).\n",
        "$$\n",
        "\n",
        "Taking the negative log gives the **negative log-likelihood**\n",
        "\n",
        "$$\n",
        "f(\\mathbf{X}) = -\\log \\mathbb{P}(b_1,\\dots,b_n \\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n, \\mathbf{X})\n",
        "= -\\sum_{i=1}^n \\log \\mathbb{P}(b_i \\mid \\mathbf{a}_i),\n",
        "$$\n",
        "\n",
        "which simplifies to\n",
        "\n",
        "$$\n",
        "f(\\mathbf{X})\n",
        "= \\sum_{i=1}^n \\left[\n",
        " -\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\n",
        " + \\log\\left(\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)\\right)\n",
        "\\right].\n",
        "$$\n",
        "\n",
        "\n",
        "This is exactly the desired form of the negative log-likelihood for multiclass softmax logistic regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(b)__ (2 points) Show that $\\mathbf{u} \\mapsto \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)$ is convex on $\\mathbb{R}^C$. Then, show that $f(\\mathbf{X})$ is convex.\n",
        "\n",
        "\n",
        "Hint: use Jensen's inequality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "#### Step 1: $\\mathrm{lse}(\\mathbf{u}) := \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)$ is convex on $\\mathbb{R}^C$.\n",
        "\n",
        "Let $\\Delta_C := \\{\\boldsymbol{\\theta}\\in\\mathbb{R}^C_{\\ge 0} : \\sum_{k=1}^C \\theta_k = 1\\}$ be the probability simplex.  \n",
        "Because $\\log$ is **concave**, Jensen‚Äôs inequality says that for any positive numbers $t_k>0$ and any $\\boldsymbol{\\theta}\\in\\Delta_C$,\n",
        "$$\n",
        "\\log\\!\\Big(\\sum_{k=1}^C \\theta_k t_k\\Big)\\;\\ge\\;\\sum_{k=1}^C \\theta_k \\log t_k,\n",
        "$$\n",
        "with equality iff $t_k$ are all equal for every index with $\\theta_k>0$.\n",
        "\n",
        "Apply this with $t_k=\\dfrac{e^{u_k}}{\\theta_k}$ (if some $\\theta_k=0$, read the expression by continuity; the inequality still holds). Then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{lse}(\\mathbf{u})\n",
        "&= \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)\n",
        "= \\log\\!\\left(\\sum_{k=1}^C \\theta_k \\frac{e^{u_k}}{\\theta_k}\\right)\n",
        "\\\\\n",
        "&\\ge \\sum_{k=1}^C \\theta_k \\log\\!\\left(\\frac{e^{u_k}}{\\theta_k}\\right)\n",
        "= \\sum_{k=1}^C \\theta_k u_k \\;-\\; \\sum_{k=1}^C \\theta_k \\log \\theta_k.\n",
        "\\end{aligned}\n",
        "\\tag{1}\n",
        "$$\n",
        "Since $\\boldsymbol{\\theta}$ was arbitrary, we obtain the variational form\n",
        "$$\n",
        "\\mathrm{lse}(\\mathbf{u})\n",
        "\\;\\ge\\;\n",
        "\\sup_{\\boldsymbol{\\theta}\\in\\Delta_C}\n",
        "\\Big\\{\\,\\boldsymbol{\\theta}^\\top \\mathbf{u} - \\sum_{k=1}^C \\theta_k \\log \\theta_k\\,\\Big\\}.\n",
        "\\tag{2}\n",
        "$$\n",
        "\n",
        "**Where does equality hold?**  \n",
        "Equality in Jensen for a concave function holds iff all inputs to $\\log$ are equal on the support of $\\boldsymbol{\\theta}$.  \n",
        "Here those inputs are $t_k=\\dfrac{e^{u_k}}{\\theta_k}$. Thus equality requires a constant $Z>0$ such that\n",
        "$$\n",
        "\\frac{e^{u_k}}{\\theta_k} = Z \\quad \\text{for all } k \\text{ with } \\theta_k>0,\n",
        "$$\n",
        "equivalently\n",
        "$$\n",
        "\\theta_k = \\frac{e^{u_k}}{Z}.\n",
        "$$\n",
        "Enforcing $\\sum_k \\theta_k=1$ gives $Z=\\sum_{j=1}^C e^{u_j}$. Therefore the maximizer is\n",
        "$$\n",
        "\\boxed{\\,\\theta_k^*(\\mathbf{u})=\\dfrac{e^{u_k}}{\\sum_{j=1}^C e^{u_j}}\\,}.\n",
        "$$\n",
        "Plugging $\\boldsymbol{\\theta}^*$ into the right-hand side of (1),\n",
        "$$\n",
        "\\sum_{k} \\theta_k^* u_k - \\sum_{k} \\theta_k^* \\log \\theta_k^*\n",
        "= \\sum_k \\theta_k^* u_k - \\sum_k \\theta_k^*(u_k - \\log Z)\n",
        "= \\log Z\n",
        "= \\log\\!\\left(\\sum_{j=1}^C e^{u_j}\\right)\n",
        "= \\mathrm{lse}(\\mathbf{u}),\n",
        "$$\n",
        "so the inequality is tight and we have the exact identity\n",
        "$$\n",
        "\\boxed{\\;\\mathrm{lse}(\\mathbf{u}) = \\sup_{\\boldsymbol{\\theta}\\in\\Delta_C}\n",
        "\\Big\\{\\,\\boldsymbol{\\theta}^\\top \\mathbf{u} - \\sum_{k=1}^C \\theta_k \\log \\theta_k\\,\\Big\\}. \\;}\n",
        "\\tag{3}\n",
        "$$\n",
        "\n",
        "For each fixed $\\boldsymbol{\\theta}$, the map $\\mathbf{u}\\mapsto \\boldsymbol{\\theta}^\\top \\mathbf{u} - \\sum_k \\theta_k \\log \\theta_k$ is **affine** in $\\mathbf{u}$. A supremum of affine functions is convex, hence $\\mathrm{lse}$ is convex on $\\mathbb{R}^C$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2: Convexity of $f(\\mathbf{X})$.\n",
        "\n",
        "From part (a),\n",
        "$$\n",
        "f(\\mathbf{X})\n",
        "= \\sum_{i=1}^n \\left[\n",
        "-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\n",
        "+ \\log\\!\\left(\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k}\\right)\n",
        "\\right]\n",
        "= \\sum_{i=1}^n \\left[\n",
        "-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\n",
        "+ \\mathrm{lse}\\!\\big(\\mathbf{u}_i(\\mathbf{X})\\big)\n",
        "\\right],\n",
        "$$\n",
        "where $\\mathbf{u}_i(\\mathbf{X}) := \\big(\\mathbf{a}_i^\\top \\mathbf{x}_1,\\dots,\\mathbf{a}_i^\\top \\mathbf{x}_C\\big)\\in\\mathbb{R}^C$.\n",
        "\n",
        "- The map $\\mathbf{X}\\mapsto \\mathbf{u}_i(\\mathbf{X})$ is **affine** in $\\mathbf{X}$.\n",
        "- Since $\\mathrm{lse}$ is convex, the composition $\\mathbf{X}\\mapsto \\mathrm{lse}\\!\\big(\\mathbf{u}_i(\\mathbf{X})\\big)$ is convex (convex $\\circ$ affine).\n",
        "- Sums of convex functions are convex, and adding the linear term $-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}$ preserves convexity.\n",
        "\n",
        "Therefore $f(\\mathbf{X})$ is convex in $\\mathbf{X}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator\n",
        "$$\n",
        "\\mathbf{X}^\\star_{ML} = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "is a global minimum. But does the minimum always exist? We will ponder this question in the following three points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(c)__ (1 point) Explain the difference between infima and minima. Give an example of a convex function on $\\mathbb{R}$ that does not attain its infimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "- **Infimum:** The infimum of $f$ over a set $S$ is the greatest lower bound of $\\{f(x): x\\in S\\}$.  \n",
        "  In symbols,\n",
        "  $$\n",
        "  \\inf_{x\\in S} f(x) \\;=\\; \\sup\\{\\alpha\\in\\mathbb{R} : \\alpha \\le f(x)\\ \\text{for all } x\\in S\\}.\n",
        "  $$\n",
        "  It need not be achieved by any $x\\in S$.\n",
        "\n",
        "- **Minimum:** A minimum is a point $x^\\star\\in S$ such that $f(x^\\star)\\le f(x)$ for all $x\\in S$; equivalently,\n",
        "  $$\n",
        "  f(x^\\star) \\;=\\; \\inf_{x\\in S} f(x),\n",
        "  $$\n",
        "  i.e., the infimum is **attained** at $x^\\star$.\n",
        "\n",
        "**Example (convex on $\\mathbb{R}$ with no minimum):**  \n",
        "Let $f(x)=e^x$. Then $f''(x)=e^x>0$, so $f$ is convex on $\\mathbb{R}$.  \n",
        "We have\n",
        "$$\n",
        "\\inf_{x\\in\\mathbb{R}} f(x) \\;=\\; 0 \\quad \\text{(since $e^x\\to 0$ as $x\\to -\\infty$),}\n",
        "$$\n",
        "but $e^x>0$ for every $x$, so there is **no** $x^\\star$ with $f(x^\\star)=0$.  \n",
        "Thus the infimum is **not attained**-$f$ has no minimizer on $\\mathbb{R}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(d)__ (1 point) Assume there exists $\\mathbf{X}_0 \\in \\mathbb{R}^{p\\times C}$ such that for all $i$,\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0.\n",
        "$$\n",
        "This is called one-versus-all complete separation in multiclass settings. Give a geometric interpretation (e.g., for $p=2$) and explain why the name is appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "Assume there exists $\\mathbf{X}_0 \\in \\mathbb{R}^{p\\times C}$ such that for every sample $i$,\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} \\;-\\; \\max_{k\\ne b_i}\\,\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} \\;>\\; 0.\n",
        "\\tag{*}\n",
        "$$\n",
        "Write the columns of $\\mathbf{X}_0$ as $\\mathbf{x}_{0,1},\\dots,\\mathbf{x}_{0,C}$ and define the **scores**\n",
        "$$\n",
        "s_c(\\mathbf{a}) := \\mathbf{a}^\\top \\mathbf{x}_{0,c}, \\qquad c=1,\\dots,C.\n",
        "$$\n",
        "The class predicted by the linear (softmax) model is $\\arg\\max_c s_c(\\mathbf{a})$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 1: Pairwise decision boundaries\n",
        "For any two classes $c$ and $k$, the boundary where they tie is\n",
        "$$\n",
        "\\{\\mathbf{a} : s_c(\\mathbf{a}) = s_k(\\mathbf{a})\\}\n",
        "\\;\\Longleftrightarrow\\;\n",
        "\\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k})=0\\}.\n",
        "$$\n",
        "This set is a hyperplane **through the origin** with normal vector $\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}$.  \n",
        "(It passes through the origin because there is no intercept; with an intercept one can augment $\\mathbf{a}$ by a $1$ to keep the same form.)\n",
        "\n",
        "Define the half-spaces\n",
        "$$\n",
        "H_{c\\succ k} := \\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}) \\ge 0\\}\n",
        "\\quad\\text{and}\\quad\n",
        "H_{k\\succ c} := \\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}) \\le 0\\}.\n",
        "$$\n",
        "Points in $H_{c\\succ k}$ are those where class $c$‚Äôs score is at least as large as class $k$‚Äôs.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2: From $(*)$ to per-class conic regions\n",
        "The condition $(*)$ is equivalent to a **family of strict pairwise inequalities**:\n",
        "\n",
        "- Since $\\max_{k\\ne b_i} s_k(\\mathbf{a}_i)$ is the largest competitor score,\n",
        "  $$\n",
        "  s_{b_i}(\\mathbf{a}_i) - \\max_{k\\ne b_i} s_k(\\mathbf{a}_i) > 0\n",
        "  \\;\\Longrightarrow\\;\n",
        "  s_{b_i}(\\mathbf{a}_i) - s_k(\\mathbf{a}_i) > 0 \\quad \\forall\\, k\\ne b_i,\n",
        "  $$\n",
        "  i.e.\n",
        "  $$\n",
        "  \\mathbf{a}_i^\\top(\\mathbf{x}_{0,b_i}-\\mathbf{x}_{0,k}) > 0 \\quad \\forall\\, k\\ne b_i.\n",
        "  \\tag{1}\n",
        "  $$\n",
        "\n",
        "- Conversely, if (1) holds for all $k\\ne b_i$, then $s_{b_i}(\\mathbf{a}_i)$ is strictly larger than **every** $s_k(\\mathbf{a}_i)$, hence larger than their maximum; thus $(*)$ holds.  \n",
        "  Therefore, $(*) \\Longleftrightarrow$ (1).\n",
        "\n",
        "For each class $c$, define the **one-versus-all region**\n",
        "$$\n",
        "\\mathcal{R}_c := \\bigcap_{k\\ne c} H_{c\\succ k}\n",
        "= \\left\\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}) \\ge 0 \\ \\ \\forall\\,k\\ne c\\right\\}.\n",
        "$$\n",
        "This is an intersection of $C-1$ half-spaces, hence a closed **convex polyhedral cone** (a wedge when $p=2$).  \n",
        "Its **interior** is\n",
        "$$\n",
        "\\operatorname{int}(\\mathcal{R}_c) = \\left\\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}) > 0 \\ \\ \\forall\\,k\\ne c\\right\\}.\n",
        "$$\n",
        "\n",
        "By (1), every data point of class $b_i$ lies in $\\operatorname{int}(\\mathcal{R}_{b_i})$ with a **strict margin** to all the pairwise hyperplanes.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 3: Geometric interpretation (take $p=2$ for a picture)\n",
        "When $p=2$, each boundary $\\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}) = 0\\}$ is a **line through the origin** with normal $\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}$.  \n",
        "These lines partition the plane into at most $C$ wedge-shaped (conic) regions. The region $\\mathcal{R}_c$ is the wedge where $s_c(\\mathbf{a})$ dominates all other scores.  \n",
        "Condition $(*)$ says each point of class $c$ lies **strictly inside** that wedge, not on any boundary.  \n",
        "Hence different classes occupy **disjoint regions** separated by these linear boundaries, with a positive margin.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 4: Why the name ‚Äúone-versus-all complete separation‚Äù?\n",
        "- **One-versus-all:** For each class $c$, the requirement is\n",
        "  $$\n",
        "  s_c(\\mathbf{a}) > s_k(\\mathbf{a}) \\quad \\text{for all } k\\ne c,\n",
        "  $$\n",
        "  i.e., the score of $c$ beats the scores of **all** other classes at every point of class $c$.\n",
        "- **Complete:** The inequalities are **strict** (‚Äú$>$‚Äù), so there are no ties; every point is strictly on the correct side of every pairwise hyperplane.\n",
        "- **Separation:** Because each class‚Äôs points lie in the interior of $\\mathcal{R}_c$, the class regions are disjoint and linearly separated by the hyperplanes\n",
        "  $$\n",
        "  \\{\\mathbf{a} : \\mathbf{a}^\\top(\\mathbf{x}_{0,c}-\\mathbf{x}_{0,k}) = 0\\}, \\qquad c\\ne k.\n",
        "  $$\n",
        "\n",
        "Thus the assumption $(*)$ exactly states **one-versus-all complete separation** in the multiclass linear (softmax) setting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this, you should see that it is likely that some datasets satisfy the complete separation assumption. Unfortunately, as you will show next, this can become an obstacle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(e)__ (1 point) In a one-versus-all complete separation setting (as in (d)), prove that $f$ does not attain its minimum. Hint: consider $f(\\alpha \\mathbf{X}_0)$ as $\\alpha \\to +\\infty$ and compare it to $f(\\mathbf{X}_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "Let $\\mathbf{X}_0=[\\mathbf{x}_{0,1},\\ldots,\\mathbf{x}_{0,C}]$ satisfy the separation condition from (d):\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} \\;-\\; \\max_{k\\ne b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} \\;>\\; 0\n",
        "\\qquad\\forall i.\n",
        "$$\n",
        "\n",
        "Define the per-example **pairwise margins**\n",
        "$$\n",
        "\\Delta_{ik} \\;:=\\; \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} \\;>\\; 0,\n",
        "\\qquad\n",
        "\\Delta_i \\;:=\\; \\min_{k\\ne b_i}\\Delta_{ik} \\;>\\; 0,\n",
        "$$\n",
        "and let $\\gamma := \\min_i \\Delta_i > 0$ (the minimum over finitely many strictly positive numbers).\n",
        "\n",
        "Consider the ray $\\mathbf{X}(\\alpha) := \\alpha \\mathbf{X}_0$ with $\\alpha>0$.  \n",
        "Using the expression from (a),\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\mathbf{X}(\\alpha))\n",
        "&= \\sum_{i=1}^n \\left[-\\,\\alpha\\,\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i}\n",
        "+ \\log\\!\\left(\\sum_{k=1}^C e^{\\alpha\\,\\mathbf{a}_i^\\top \\mathbf{x}_{0,k}}\\right)\\right] \\\\\n",
        "&= \\sum_{i=1}^n \\log\\!\\left(\\sum_{k=1}^C e^{\\alpha(\\mathbf{a}_i^\\top \\mathbf{x}_{0,k}-\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i})}\\right) \\\\\n",
        "&= \\sum_{i=1}^n \\log\\!\\left( 1 + \\sum_{k\\ne b_i} e^{-\\alpha\\,\\Delta_{ik}} \\right).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Upper bound and limit.**  \n",
        "Because $\\Delta_{ik}\\ge \\Delta_i \\ge \\gamma$,\n",
        "$$\n",
        "f(\\mathbf{X}(\\alpha))\n",
        "\\;\\le\\; \\sum_{i=1}^n \\log\\!\\left(1 + (C-1)e^{-\\alpha \\Delta_i}\\right)\n",
        "\\;\\le\\; n\\,\\log\\!\\left(1 + (C-1)e^{-\\alpha \\gamma}\\right)\n",
        "\\;\\xrightarrow[\\alpha\\to\\infty]{}\\; 0.\n",
        "$$\n",
        "\n",
        "**Lower bound (nonnegativity).**  \n",
        "For any $\\mathbf{X}$,\n",
        "$$\n",
        "-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log\\!\\left(\\sum_k e^{\\mathbf{a}_i^\\top \\mathbf{x}_k}\\right)\n",
        "\\;\\ge\\; -\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log\\!\\left(e^{\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}}\\right)\n",
        "\\;=\\; 0,\n",
        "$$\n",
        "so $f(\\mathbf{X})\\ge 0$ for all $\\mathbf{X}$. Hence $\\inf_{\\mathbf{X}} f(\\mathbf{X}) = 0$.\n",
        "\n",
        "**Non-attainment.**  \n",
        "If $f$ attained its minimum at some finite $\\mathbf{X}^\\star$, we would have $f(\\mathbf{X}^\\star)=0$.  \n",
        "Since each summand is nonnegative, each must equal $0$, i.e. for every $i$,\n",
        "$$\n",
        "0 \\;=\\; \\log\\!\\left(\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k^\\star}\\right)\n",
        "- \\mathbf{a}_i^\\top \\mathbf{x}_{b_i}^\\star\n",
        "\\quad\\Longrightarrow\\quad\n",
        "\\sum_{k\\ne b_i} e^{\\mathbf{a}_i^\\top \\mathbf{x}_k^\\star} \\;=\\; 0,\n",
        "$$\n",
        "which is impossible because every term $e^{\\mathbf{a}_i^\\top \\mathbf{x}_k^\\star}$ is strictly positive.  \n",
        "Therefore no finite $\\mathbf{X}$ achieves $f(\\mathbf{X})=0$, and the minimum is **not attained**.\n",
        "\n",
        "**Conclusion.**  \n",
        "Under complete separation, the negative log-likelihood has infimum $0$ but **no minimizer**.  \n",
        "Along the ray $\\mathbf{X}(\\alpha)=\\alpha\\mathbf{X}_0$ the objective decreases to $0$ as $\\alpha\\to\\infty$, reflecting that the model pushes class probabilities to $1$ for the observed labels while parameters diverge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We resolve this issue by adding a regularizer. Consider the regularized function\n",
        "\n",
        "$$\n",
        " f_\\mu(\\mathbf{X}) = f(\\mathbf{X}) + \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2, \\quad \\mu > 0.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(f)__ (1 point) Show that the gradient with respect to $\\mathbf{X}$ of $f_\\mu$ can be expressed as\n",
        "$$\n",
        " \\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n \\mathbf{a}_i \\big( \\mathbf{p}_i - \\mathbf{e}_{b_i} \\big)^\\top + \\mu \\mathbf{X},\\tag{1}\n",
        "$$\n",
        "where $\\mathbf{e}_{b_i} \\in \\mathbb{R}^C$ is the [one-hot vector](https://en.wikipedia.org/wiki/One-hot) for class $b_i$, $\\mathbf{p}_i \\in \\mathbb{R}^C$ has entries $p_{i,c} = \\mathbb{P}(b_i=c\\mid \\mathbf{a}_i)$ under the softmax model, and $(\\mathbf{p}_i - \\mathbf{e}_{b_i})\\mathbf{a}_i^\\top$ denotes the outer product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "We regularize the multiclass softmax NLL with an $\\ell_2$ term:\n",
        "$$\n",
        "f_\\mu(\\mathbf{X}) \\;=\\; f(\\mathbf{X}) \\;+\\; \\frac{\\mu}{2}\\,\\|\\mathbf{X}\\|_F^2,\n",
        "\\qquad \\mu>0,\n",
        "$$\n",
        "where the unregularized loss is\n",
        "$$\n",
        "f(\\mathbf{X}) \\;=\\; \\sum_{i=1}^n \\Bigg[\n",
        "-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\n",
        "+ \\log\\!\\left(\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k}\\right)\n",
        "\\Bigg],\n",
        "$$\n",
        "and $\\mathbf{x}_c$ is column $c$ of $\\mathbf{X}\\in\\mathbb{R}^{p\\times C}$.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1) Re-parameterize one example\n",
        "For a single example $i$, define\n",
        "$$\n",
        "\\mathbf{z}_i \\;=\\; \\mathbf{X}^\\top \\mathbf{a}_i \\in \\mathbb{R}^C,\n",
        "\\qquad\n",
        "\\mathbf{p}_i \\;=\\; \\mathrm{softmax}(\\mathbf{z}_i) \\in \\mathbb{R}^C,\n",
        "$$\n",
        "and let $\\mathbf{e}_{b_i}\\in\\mathbb{R}^C$ be the one-hot vector of the true class $b_i$.\n",
        "The $i$-th contribution to the loss can be written as\n",
        "$$\n",
        "\\phi_i(\\mathbf{X})\n",
        "\\;=\\;\n",
        "-\\,\\mathbf{e}_{b_i}^\\top \\mathbf{z}_i \\;+\\; \\mathrm{lse}(\\mathbf{z}_i),\n",
        "\\qquad\n",
        "\\text{where}\\quad\n",
        "\\mathrm{lse}(\\mathbf{z})=\\log\\!\\sum_{k=1}^C e^{z_k}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 2) Gradient with respect to $\\mathbf{z}_i$\n",
        "A standard fact (or by componentwise differentiation) is\n",
        "$$\n",
        "\\nabla_{\\mathbf{z}}\\,\\mathrm{lse}(\\mathbf{z}) \\;=\\; \\mathrm{softmax}(\\mathbf{z}).\n",
        "$$\n",
        "Therefore\n",
        "$$\n",
        "\\nabla_{\\mathbf{z}_i}\\,\\phi_i(\\mathbf{X})\n",
        "\\;=\\;\n",
        "-\\,\\mathbf{e}_{b_i} + \\mathrm{softmax}(\\mathbf{z}_i)\n",
        "\\;=\\;\n",
        "\\mathbf{p}_i - \\mathbf{e}_{b_i}\n",
        "\\;\\in\\; \\mathbb{R}^C.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 3) Chain rule to $\\mathbf{X}$\n",
        "Since $\\mathbf{z}_i=\\mathbf{X}^\\top \\mathbf{a}_i$, a variation $d\\mathbf{X}$ yields $d\\mathbf{z}_i=(d\\mathbf{X})^\\top \\mathbf{a}_i$.\n",
        "Thus, in matrix form,\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}}\\,\\phi_i(\\mathbf{X})\n",
        "\\;=\\;\n",
        "\\mathbf{a}_i\\,(\\mathbf{p}_i - \\mathbf{e}_{b_i})^\\top,\n",
        "$$\n",
        "which is an outer product in $\\mathbb{R}^{p\\times C}$. Summing over $i$ gives\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}}\\,f(\\mathbf{X})\n",
        "\\;=\\;\n",
        "\\sum_{i=1}^n \\mathbf{a}_i\\,(\\mathbf{p}_i - \\mathbf{e}_{b_i})^\\top.\n",
        "$$\n",
        "\n",
        "*(Equivalently, if $\\mathbf{A}=[\\mathbf{a}_1,\\ldots,\\mathbf{a}_n]\\in\\mathbb{R}^{p\\times n}$ and\n",
        "$\\mathbf{P},\\mathbf{Y}\\in\\mathbb{R}^{n\\times C}$ stack $\\mathbf{p}_i^\\top$ and $\\mathbf{e}_{b_i}^\\top$ as rows,\n",
        "then $\\nabla_{\\mathbf{X}} f(\\mathbf{X}) = \\mathbf{A}(\\mathbf{P}-\\mathbf{Y})$.)*\n",
        "\n",
        "---\n",
        "\n",
        "#### 4) Add the $\\ell_2$ regularizer\n",
        "Using $\\dfrac{d}{d\\mathbf{X}}\\,\\tfrac{1}{2}\\|\\mathbf{X}\\|_F^2=\\mathbf{X}$,\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}}\\!\\left(\\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2\\right) \\;=\\; \\mu\\,\\mathbf{X}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Final result\n",
        "$$\n",
        "\\boxed{\\displaystyle\n",
        "\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\n",
        "\\;=\\;\n",
        "\\sum_{i=1}^n \\mathbf{a}_i\\,(\\mathbf{p}_i - \\mathbf{e}_{b_i})^\\top\n",
        "\\;+\\; \\mu\\,\\mathbf{X}}\n",
        "$$\n",
        "where $p_{i,c} = \\mathbb{P}(b_i=c \\mid \\mathbf{a}_i)$ under the softmax model, and $(\\mathbf{p}_i - \\mathbf{e}_{b_i})\\,\\mathbf{a}_i^\\top$ denotes the outer product.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be written as\n",
        "$$\n",
        " \\nabla^2 f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I},\\tag{2}\n",
        "$$\n",
        "where $\\otimes$ is the Kronecker product, and $\\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top$ is the softmax Jacobian, which is positive semidefinite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "We differentiate the gradient from (f):\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\n",
        "=\\sum_{i=1}^n \\mathbf{a}_i(\\mathbf{p}_i-\\mathbf{e}_{b_i})^\\top + \\mu \\mathbf{X},\n",
        "\\qquad\n",
        "\\mathbf{p}_i=\\mathrm{softmax}(\\mathbf{X}^\\top\\mathbf{a}_i).\n",
        "$$\n",
        "Only the term $\\sum_i \\mathbf{a}_i\\mathbf{p}_i^\\top$ depends on $\\mathbf{X}$.\n",
        "\n",
        "Let $ \\mathbf{z}_i=\\mathbf{X}^\\top\\mathbf{a}_i\\in\\mathbb{R}^C $ and\n",
        "$ J_i:=\\nabla_{\\mathbf{z}_i}\\mathbf{p}_i=\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top\\in\\mathbb{R}^{C\\times C} $\n",
        "(the softmax Jacobian).\n",
        "\n",
        "---\n",
        "\n",
        "#### Differential $\\to$ linear-operator form\n",
        "Take differentials:\n",
        "$$\n",
        "d\\mathbf{z}_i=(d\\mathbf{X})^\\top \\mathbf{a}_i,\n",
        "\\qquad\n",
        "d\\mathbf{p}_i=J_i\\,d\\mathbf{z}_i=J_i (d\\mathbf{X})^\\top \\mathbf{a}_i.\n",
        "$$\n",
        "Therefore\n",
        "$$\n",
        "d\\!\\big(\\mathbf{a}_i\\mathbf{p}_i^\\top\\big)\n",
        "=\n",
        "\\mathbf{a}_i (d\\mathbf{p}_i)^\\top\n",
        "=\n",
        "\\mathbf{a}_i\\big((d\\mathbf{z}_i)^\\top J_i^\\top\\big)\n",
        "=\n",
        "\\mathbf{a}_i(\\mathbf{a}_i^\\top d\\mathbf{X}) J_i\n",
        "=\n",
        "(\\mathbf{a}_i\\mathbf{a}_i^\\top)\\, d\\mathbf{X}\\, J_i .\n",
        "$$\n",
        "Summing over $i$ and adding the regularizer,\n",
        "$$\n",
        "d\\!\\big(\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\\big)\n",
        "=\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\, d\\mathbf{X}\\, J_i + \\mu\\, d\\mathbf{X}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Vectorization (Kronecker form)\n",
        "Using $\\mathrm{vec}(A D B)=(B^\\top\\!\\otimes A)\\,\\mathrm{vec}(D)$ and $J_i^\\top=J_i$,\n",
        "$$\n",
        "\\mathrm{vec}\\!\\Big(d\\!\\big(\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\\big)\\Big)\n",
        "=\n",
        "\\Bigg[\\sum_{i=1}^n \\big(J_i \\otimes (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\big) + \\mu I\\Bigg]\\mathrm{vec}(d\\mathbf{X})\n",
        "=\n",
        "\\Bigg[\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes J_i + \\mu I\\Bigg]\\mathrm{vec}(d\\mathbf{X}).\n",
        "$$\n",
        "\n",
        "Hence the Hessian with respect to $\\mathbf{X}$ (viewed via $\\mathrm{vec}$) is\n",
        "$$\n",
        "\\boxed{\\;\n",
        "\\nabla^2 f_\\mu(\\mathbf{X})\n",
        "=\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\otimes\\big(\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top\\big) + \\mu I \\; }.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Softmax Jacobian is PSD\n",
        "For any $v\\in\\mathbb{R}^C$,\n",
        "$$\n",
        "v^\\top\\!\\big(\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top\\big)v\n",
        "=\\sum_{c} p_{i,c} v_c^2 - \\Big(\\sum_{c} p_{i,c} v_c\\Big)^2\n",
        "=\\mathrm{Var}_{c\\sim \\mathbf{p}_i}[v_c]\\;\\ge 0.\n",
        "$$\n",
        "Thus each summand is positive semidefinite; adding $\\mu I$ with $\\mu>0$ makes the Hessian positive definite, confirming strong convexity of $f_\\mu$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "From (g), for any $\\mathbf{X}$,\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf{X})\n",
        "\\;=\\;\n",
        "\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\;\\otimes\\;\n",
        "\\big(\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top\\big)\n",
        "\\;+\\; \\mu I .\n",
        "$$\n",
        "\n",
        "Each matrix $\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top$ is positive semidefinite (it is the softmax Jacobian).  \n",
        "Since $(\\mathbf{a}_i\\mathbf{a}_i^\\top)\\succeq 0$ as well and $A\\succeq 0$, $B\\succeq 0 \\Rightarrow A\\otimes B\\succeq 0$, the Kronecker term\n",
        "$$\n",
        "\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\otimes\\big(\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top\\big)\n",
        "$$\n",
        "is positive semidefinite. Therefore, for any vector $v$,\n",
        "$$\n",
        "v^\\top \\nabla^2 f_\\mu(\\mathbf{X}) v\n",
        "\\;\\ge\\;\n",
        "v^\\top (\\mu I) v\n",
        "\\;=\\; \\mu \\|v\\|_2^2 .\n",
        "$$\n",
        "Equivalently, $\\nabla^2 f_\\mu(\\mathbf{X}) \\succeq \\mu I$ for all $\\mathbf{X}$.\n",
        "\n",
        "A twice-differentiable function whose Hessian is uniformly bounded below by $\\mu I$ is $\\mu$-strongly convex (with respect to the Frobenius norm for matrix arguments). Hence, for all $\\mathbf{X},\\mathbf{Y}$,\n",
        "$$\n",
        "f_\\mu(\\mathbf{Y})\n",
        "\\;\\ge\\;\n",
        "f_\\mu(\\mathbf{X})\n",
        "\\;+\\;\n",
        "\\langle \\nabla f_\\mu(\\mathbf{X}),\\, \\mathbf{Y}-\\mathbf{X} \\rangle\n",
        "\\;+\\;\n",
        "\\frac{\\mu}{2}\\,\\|\\mathbf{Y}-\\mathbf{X}\\|_F^2 .\n",
        "$$\n",
        "\n",
        "Therefore, $f_\\mu$ is $\\mu$-strongly convex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(i)__ (1 point) Is it possible for a strongly convex function to not attain its minimum? Justify your reasoning (you may assume the domain is $\\mathbb{R}^{p\\times C}$).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "**Answer:** No. On $\\mathbb{R}^{p\\times C}$ a $\\mu$-strongly convex function always **attains** (and has a **unique**) minimizer.\n",
        "\n",
        "**Why.** By $\\mu$-strong convexity, for any fixed $Y$ and all $X$,\n",
        "$$\n",
        "f(X)\\;\\ge\\; f(Y)+\\langle g_Y,\\,X-Y\\rangle+\\frac{\\mu}{2}\\|X-Y\\|_F^2\n",
        "\\qquad (g_Y\\in\\partial f(Y)).\n",
        "$$\n",
        "Take $Y=0$ and some $g_0\\in\\partial f(0)$. Then\n",
        "$$\n",
        "f(X)\\;\\ge\\; f(0)+\\langle g_0, X\\rangle+\\frac{\\mu}{2}\\|X\\|_F^2\n",
        "\\;\\ge\\; \\frac{\\mu}{2}\\|X\\|_F^2-\\|g_0\\|_F\\,\\|X\\|_F + f(0).\n",
        "$$\n",
        "The right-hand side $\\to +\\infty$ as $\\|X\\|_F\\to\\infty$; thus **$f$ is coercive**.\n",
        "\n",
        "A convex function finite everywhere is lower semicontinuous; in finite dimensions, a lower semicontinuous **coercive** function **attains its minimum** (e.g., any minimizing sequence is bounded by coercivity, then compactness and lower semicontinuity yield a limit minimizer). Hence $f$ attains a minimum.\n",
        "\n",
        "Moreover, strong convexity implies **uniqueness**: if $X_1\\neq X_2$ both minimized $f$, strong convexity would make the midpoint have strictly smaller value, a contradiction.\n",
        "\n",
        "Therefore, on $\\mathbb{R}^{p\\times C}$, a strongly convex function **cannot** fail to attain its minimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now show that $f_\\mu$ is smooth, i.e., $\\nabla f_\\mu$ is L-Lipschitz with respect to the Frobenius norm, with a simple conservative bound\n",
        "$$\n",
        " L = \\|\\mathbf{A}\\|_F^2 + \\mu.\n",
        "$$\n",
        "where\n",
        "$$\n",
        " \\mathbf{A} = \\begin{bmatrix}\n",
        "  \\leftarrow &  \\mathbf{a}_1^\\top & \\rightarrow \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_2^\\top & \\rightarrow \\\\\n",
        "   &  \\ldots &  \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_n^\\top & \\rightarrow \\\\\n",
        " \\end{bmatrix}.\n",
        "$$\n",
        "(You may use that the operator norm of the softmax Jacobian is bounded by 1/4, and a looser bound $\\le 1$ is acceptable for grading.)\n",
        "\n",
        "Hint: check the properties of the spectral norm with respect to dot product, Kronecker product, and outer product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 point for all three questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "**Claim.** For any $\\mathbf{a}_i\\in\\mathbb{R}^p$,\n",
        "$$\n",
        "\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^\\top)=\\|\\mathbf{a}_i\\|_2^2.\n",
        "$$\n",
        "\n",
        "**Proof.** The matrix $\\mathbf{a}_i\\mathbf{a}_i^\\top$ is symmetric, positive semidefinite, and rank one.\n",
        "\n",
        "1) **Exhibit an eigenpair.**  \n",
        "   Compute\n",
        "   $$\n",
        "   (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\mathbf{a}_i\n",
        "   = \\mathbf{a}_i(\\mathbf{a}_i^\\top\\mathbf{a}_i)\n",
        "   = \\|\\mathbf{a}_i\\|_2^2\\,\\mathbf{a}_i,\n",
        "   $$\n",
        "   so $\\mathbf{a}_i$ is an eigenvector with eigenvalue $\\|\\mathbf{a}_i\\|_2^2$.\n",
        "\n",
        "2) **Upper bound any eigenvalue by the Rayleigh quotient.**  \n",
        "   For any unit vector $\\mathbf{x}$,\n",
        "   $$\n",
        "   \\mathbf{x}^\\top(\\mathbf{a}_i\\mathbf{a}_i^\\top)\\mathbf{x}\n",
        "   = (\\mathbf{a}_i^\\top\\mathbf{x})^2\n",
        "   \\le \\|\\mathbf{a}_i\\|_2^2\\|\\mathbf{x}\\|_2^2\n",
        "   = \\|\\mathbf{a}_i\\|_2^2,\n",
        "   $$\n",
        "   where the inequality is Cauchy-Schwarz. Hence $\\lambda_{\\max}\\le \\|\\mathbf{a}_i\\|_2^2$.\n",
        "\n",
        "Combining with step 1 (which shows $\\lambda_{\\max}\\ge \\|\\mathbf{a}_i\\|_2^2$), we obtain equality:\n",
        "$$\n",
        "\\boxed{\\ \\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^\\top)=\\|\\mathbf{a}_i\\|_2^2\\ }.\n",
        "$$\n",
        "\n",
        "(Every other eigenvalue equals $0$ because $\\mathbf{a}_i\\mathbf{a}_i^\\top$ is rank one.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-2)__ Using (2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "From (g),\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf{X})\n",
        "=\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\otimes J_i + \\mu I,\n",
        "\\qquad\n",
        "J_i=\\mathrm{Diag}(\\mathbf{p}_i)-\\mathbf{p}_i\\mathbf{p}_i^\\top \\succeq 0 .\n",
        "$$\n",
        "\n",
        "For PSD matrices, $\\lambda_{\\max}(M)=\\|M\\|_2$. Using standard spectral-norm properties:\n",
        "\n",
        "1. **Sum:** $\\| \\sum_i M_i \\|_2 \\le \\sum_i \\|M_i\\|_2$.\n",
        "2. **Kronecker:** $\\|A\\otimes B\\|_2=\\|A\\|_2\\,\\|B\\|_2$.\n",
        "3. **Rank-one:** from (j-1), $\\|\\mathbf{a}_i\\mathbf{a}_i^\\top\\|_2=\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^\\top)=\\|\\mathbf{a}_i\\|_2^2$.\n",
        "4. **Softmax Jacobian:** $\\|J_i\\|_2 \\le 1$ (tighter bound $\\le\\frac14$ also holds).\n",
        "\n",
        "Hence, for each $i$,\n",
        "$$\n",
        "\\|(\\mathbf{a}_i\\mathbf{a}_i^\\top)\\otimes J_i\\|_2\n",
        "=\\|\\mathbf{a}_i\\mathbf{a}_i^\\top\\|_2\\,\\|J_i\\|_2\n",
        "\\le \\|\\mathbf{a}_i\\|_2^2.\n",
        "$$\n",
        "Therefore,\n",
        "$$\n",
        "\\lambda_{\\max}\\!\\big(\\nabla^2 f_\\mu(\\mathbf{X})\\big)\n",
        "= \\Big\\|\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top)\\otimes J_i + \\mu I \\Big\\|_2\n",
        "\\le \\sum_{i=1}^n \\|(\\mathbf{a}_i\\mathbf{a}_i^\\top)\\otimes J_i\\|_2 + \\|\\mu I\\|_2\n",
        "\\le \\sum_{i=1}^n \\|\\mathbf{a}_i\\|_2^2 + \\mu.\n",
        "$$\n",
        "\n",
        "*(If you use the tighter bound $\\|J_i\\|_2 \\le \\tfrac14$, you obtain\n",
        "$\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\le \\tfrac14 \\sum_i \\|\\mathbf{a}_i\\|_2^2 + \\mu$.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|\\mathbf{A}\\|_F^2 + \\mu$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "From (j-2), for every $\\mathbf{X}$,\n",
        "$$\n",
        "\\lambda_{\\max}\\!\\big(\\nabla^2 f_\\mu(\\mathbf{X})\\big)\\;\\le\\;\\sum_{i=1}^n \\|\\mathbf{a}_i\\|_2^2+\\mu .\n",
        "$$\n",
        "Let $\\mathbf{A}\\in\\mathbb{R}^{n\\times p}$ have rows $\\mathbf{a}_i^\\top$. Then\n",
        "$$\n",
        "\\sum_{i=1}^n \\|\\mathbf{a}_i\\|_2^2 \\;=\\; \\|\\mathbf{A}\\|_F^2 ,\n",
        "$$\n",
        "so\n",
        "$$\n",
        "\\sup_{\\mathbf{X}} \\lambda_{\\max}\\!\\big(\\nabla^2 f_\\mu(\\mathbf{X})\\big) \\;\\le\\; \\|\\mathbf{A}\\|_F^2+\\mu .\n",
        "$$\n",
        "\n",
        "For a twice-differentiable function, the Lipschitz constant of the gradient (with respect to $\\|\\cdot\\|_F$) can be taken as any upper bound on $\\sup_{\\mathbf{X}}\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X}))$. Hence, for all $\\mathbf{X},\\mathbf{Y}$,\n",
        "$$\n",
        "\\|\\nabla f_\\mu(\\mathbf{X})-\\nabla f_\\mu(\\mathbf{Y})\\|_F\n",
        "\\;\\le\\;\n",
        "\\big(\\sup_{\\mathbf{Z}}\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{Z}))\\big)\\,\\|\\mathbf{X}-\\mathbf{Y}\\|_F\n",
        "\\;\\le\\; (\\|\\mathbf{A}\\|_F^2+\\mu)\\,\\|\\mathbf{X}-\\mathbf{Y}\\|_F .\n",
        "$$\n",
        "\n",
        "Therefore, $f_\\mu$ is $L$-smooth with\n",
        "$$\n",
        "\\boxed{\\,L=\\|\\mathbf{A}\\|_F^2+\\mu\\, }.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(l)__ (1 point) KL divergence and NLL. Let $q(b_i\\mid\\mathbf{a}_i)$ be the true label distribution and $p(b_i\\mid\\mathbf{a}_i)$ the model softmax. Write the KL divergence $\\mathrm{KL}(q\\,\\|\\,p)$ and show that minimizing the KL divergence between $q$ and $p$ is equivalent to minimizing the negative log-likelihood derived in (a).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "Let $p_i(c)=\\mathbb{P}(b_i=c\\mid \\mathbf{a}_i;\\mathbf{X})$ be the model softmax for example $i$ and\n",
        "$q_i(c)=q(b_i=c\\mid \\mathbf{a}_i)$ the (unknown/true) label distribution.\n",
        "\n",
        "The KL divergence between $q$ and $p$ over the dataset is\n",
        "$$\n",
        "\\sum_{i=1}^n \\mathrm{KL}(q_i\\|p_i)\n",
        "=\\sum_{i=1}^n \\sum_{c=1}^C q_i(c)\\,\\log\\frac{q_i(c)}{p_i(c)}\n",
        "= -\\underbrace{\\sum_{i=1}^n \\sum_{c=1}^C q_i(c)\\log p_i(c)}_{\\text{cross-entropy }H(q,p)}\n",
        "\\;+\\; \\underbrace{\\sum_{i=1}^n \\sum_{c=1}^C q_i(c)\\log q_i(c)}_{-\\sum_i H(q_i)} .\n",
        "$$\n",
        "The second term depends only on $q$ (not on $\\mathbf{X}$), hence it is a constant for optimization over $\\mathbf{X}$.\n",
        "Therefore,\n",
        "$$\n",
        "\\arg\\min_{\\mathbf{X}} \\sum_{i=1}^n \\mathrm{KL}(q_i\\|p_i)\n",
        "\\;=\\; \\arg\\min_{\\mathbf{X}} H(q,p)\n",
        "\\;=\\; \\arg\\max_{\\mathbf{X}} \\sum_{i=1}^n \\sum_{c=1}^C q_i(c)\\log p_i(c).\n",
        "$$\n",
        "\n",
        "**Hard labels (one-hot).**  \n",
        "If each training example has a single observed label $b_i$, then\n",
        "$q_i(c)=\\mathbf{1}\\{c=b_i\\}=\\mathbf{e}_{b_i,c}$, and the cross-entropy becomes\n",
        "$$\n",
        "H(q,p) \\;=\\; -\\sum_{i=1}^n \\log p_i(b_i).\n",
        "$$\n",
        "Under the softmax model,\n",
        "$$\n",
        "-\\sum_{i=1}^n \\log p_i(b_i)\n",
        "= \\sum_{i=1}^n \\left[\\log\\!\\left(\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k}\\right)\n",
        "- \\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\\right],\n",
        "$$\n",
        "which is exactly the negative log-likelihood from part (a).\n",
        "\n",
        "**Conclusion.** Minimizing $\\sum_i \\mathrm{KL}(q_i\\|p_i)$ is equivalent to minimizing the NLL (cross-entropy) used in multiclass logistic regression; for one-hot $q$, the KL equals the NLL up to the additive constant $\\sum_i H(q_i)$ (which is $0$ for one-hot labels).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From your work in this section, you have shown that the maximum likelihood estimator for multiclass softmax logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_F^2$ regularizer. Consequently, the estimator for $\\mathbf{X}$ we will use will be the solution of the smooth strongly convex problem,\n",
        "$$\n",
        " \\mathbf{X}^\\star = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X}) + \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2.\\tag{3}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binary logistic regression (specialization for Part 2)\n",
        "\n",
        "While this part analyzed the multiclass (softmax) setting, in the next exercise we will continue under the simplified two-class case.\n",
        "\n",
        "Let labels be $b_i \\in \\{-1, +1\\}$, features $\\mathbf{a}_i \\in \\mathbb{R}^p$, and weight vector $\\mathbf{x} \\in \\mathbb{R}^p$. Define the sigmoid\n",
        "$$\n",
        "\\sigma(t) = \\frac{1}{1+e^{-t}}.\n",
        "$$\n",
        "Model the conditional distribution as\n",
        "$$\n",
        "\\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma\\big(j\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big), \\quad j \\in \\{-1,+1\\}.\n",
        "$$\n",
        "The likelihood over i.i.d. samples $\\{(\\mathbf{a}_i, b_i)\\}_{i=1}^n$ is\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\prod_{i=1}^n \\sigma\\big(b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big),\n",
        "$$\n",
        "so the negative log-likelihood is\n",
        "$$\n",
        " f(\\mathbf{x}) = -\\log \\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^n \\log\\big(1 + e^{-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
        "$$\n",
        "\n",
        "__(m)__ (1 point) Show that the gradient of the negative log-likelihood is the standard binary logistic regression gradient:\n",
        "$$\n",
        "\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\big(-b_i\\, \\sigma(-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x})\\big)\\, \\mathbf{a}_i.\n",
        "$$\n",
        "(Hint: use the chain rule and $\\sigma'(t) = \\sigma(t)\\big(1-\\sigma(t)\\big)$.)\n",
        "\n",
        "We will use this binary formulation in Part 2 - First order methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "Let labels $b_i\\in\\{-1,+1\\}$, features $\\mathbf{a}_i\\in\\mathbb{R}^p$, weights $\\mathbf{x}\\in\\mathbb{R}^p$, and\n",
        "$$\n",
        "f(\\mathbf{x}) \\;=\\; \\sum_{i=1}^n \\log\\!\\big(1+e^{-\\,b_i\\,\\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
        "$$\n",
        "\n",
        "For each term define\n",
        "$$\n",
        "t_i(\\mathbf{x}) := -\\,b_i\\,\\mathbf{a}_i^\\top \\mathbf{x},\n",
        "\\qquad\n",
        "g(t) := \\log(1+e^{t}),\n",
        "$$\n",
        "so $f(\\mathbf{x})=\\sum_i g\\!\\big(t_i(\\mathbf{x})\\big)$.\n",
        "\n",
        "- $g'(t)=\\dfrac{e^{t}}{1+e^{t}}=\\sigma(t)$, where $\\sigma(t)=\\dfrac{1}{1+e^{-t}}$.\n",
        "- $\\nabla_{\\mathbf{x}} t_i(\\mathbf{x}) = -\\,b_i\\,\\mathbf{a}_i$.\n",
        "\n",
        "By the chain rule,\n",
        "$$\n",
        "\\nabla f(\\mathbf{x})\n",
        "= \\sum_{i=1}^n g'\\!\\big(t_i(\\mathbf{x})\\big)\\,\\nabla t_i(\\mathbf{x})\n",
        "= \\sum_{i=1}^n \\sigma\\!\\big(t_i(\\mathbf{x})\\big)\\,(-\\,b_i\\,\\mathbf{a}_i)\n",
        "= \\sum_{i=1}^n \\big[-\\,b_i\\,\\sigma(-\\,b_i\\,\\mathbf{a}_i^\\top \\mathbf{x})\\big]\\mathbf{a}_i.\n",
        "$$\n",
        "\n",
        "This is the standard binary logistic-regression gradient.\n",
        "\n",
        "**Equivalent 0/1-label form.**  \n",
        "If $y_i=\\dfrac{b_i+1}{2}\\in\\{0,1\\}$, then using $\\sigma(-b_i z)=\\sigma(z)-y_i$ we get\n",
        "$$\n",
        "\\nabla f(\\mathbf{x}) \\;=\\; \\sum_{i=1}^n \\big(\\sigma(\\mathbf{a}_i^\\top \\mathbf{x})-y_i\\big)\\,\\mathbf{a}_i.\n",
        "$$\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "md-hw",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
