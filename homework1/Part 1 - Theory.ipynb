{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1\n",
        "\n",
        "#### EE-556 Mathematics of Data - Fall 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we consider a multiclass classification task modeled by multinomial (softmax) logistic regression. Your goal will be to analyze the estimator and its properties (convexity, existence/uniqueness), and to derive gradients/Hessians and smoothness bounds. The first part consists of theoretical questions only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "  ‚ÑπÔ∏è <strong>Information on group based work:</strong>\n",
        "</div>\n",
        "\n",
        "- You are to deliver only 1 notebook per group.\n",
        "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
        "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know.\n",
        "- Only one member of the group is allowed to use AI. We will require sharing the conversation history with the AI in the form of a public link. If you use multiple conversations across the same or multiple tools please share all of them. Name the person in your group who is allowed to use AI. We encourage you to use the AI to help you understand the material, but we ask you to write the code and theory solutions by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
        "  ‚ö†Ô∏è Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Person 1 **Name**: || Person 1 **SCIPER**:\n",
        "\n",
        "\n",
        "Person 2 **Name**: || Person 2 **SCIPER**:\n",
        "\n",
        "\n",
        "Person 3 **Name**: || Person 3 **SCIPER**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #0a0; background-color: #dfd; padding: 10px; border-radius: 5px;\">\n",
        "  üìì Feedback on AI use: Please use the following cell to provide feedback on the AI use in this notebook.\n",
        "  \n",
        "  For example, how useful were the tools to you? Which tools did you use? Did you feel like they helped you understand the material better?\n",
        "</div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Multiclass Softmax Logistic Regression - 15 Points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now model multiclass classification with classes $c \\in \\{1,\\dots,C\\}$. For each sample $(\\mathbf{a}_i, b_i)$ with $\\mathbf{a}_i \\in \\mathbb{R}^p$ and $b_i \\in \\{1,\\dots,C\\}$, let $\\mathbf{X} = [\\mathbf{x}_1,\\dots,\\mathbf{x}_C] \\in \\mathbb{R}^{p\\times C}$ be the class weight matrix. The softmax model defines\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(b_i = c \\mid \\mathbf{a}_i) = \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_c)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}.\n",
        "$$\n",
        "\n",
        "Assume i.i.d. samples $\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n$. Our goal is to estimate $\\mathbf{X}$ by maximum likelihood (and later with an $\\ell_2$ regularizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(a)__ (1 point) Show that the negative log-likelihood $f$ can be written as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        " f(\\mathbf{X})\n",
        " &= - \\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
        " &= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(b)__ (2 points) Show that $\\mathbf{u} \\mapsto \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)$ is convex on $\\mathbb{R}^C$. Then, show that $f(\\mathbf{X})$ is convex.\n",
        "\n",
        "\n",
        "Hint: use Jensen's inequality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator\n",
        "$$\n",
        "\\mathbf{X}^\\star_{ML} = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "is a global minimum. But does the minimum always exist? We will ponder this question in the following three points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(c)__ (1 point) Explain the difference between infima and minima. Give an example of a convex function on $\\mathbb{R}$ that does not attain its infimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(d)__ (1 point) Assume there exists $\\mathbf{X}_0 \\in \\mathbb{R}^{p\\times C}$ such that for all $i$,\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0.\n",
        "$$\n",
        "This is called one-versus-all complete separation in multiclass settings. Give a geometric interpretation (e.g., for $p=2$) and explain why the name is appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this, you should see that it is likely that some datasets satisfy the complete separation assumption. Unfortunately, as you will show next, this can become an obstacle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(e)__ (1 point) In a one-versus-all complete separation setting (as in (d)), prove that $f$ does not attain its minimum. Hint: consider $f(\\alpha \\mathbf{X}_0)$ as $\\alpha \\to +\\infty$ and compare it to $f(\\mathbf{X}_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We resolve this issue by adding a regularizer. Consider the regularized function\n",
        "\n",
        "$$\n",
        " f_\\mu(\\mathbf{X}) = f(\\mathbf{X}) + \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2, \\quad \\mu > 0.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(f)__ (1 point) Show that the gradient with respect to $\\mathbf{X}$ of $f_\\mu$ can be expressed as\n",
        "$$\n",
        " \\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n \\mathbf{a}_i \\big( \\mathbf{p}_i - \\mathbf{e}_{b_i} \\big)^\\top + \\mu \\mathbf{X},\\tag{1}\n",
        "$$\n",
        "where $\\mathbf{e}_{b_i} \\in \\mathbb{R}^C$ is the [one-hot vector](https://en.wikipedia.org/wiki/One-hot) for class $b_i$, $\\mathbf{p}_i \\in \\mathbb{R}^C$ has entries $p_{i,c} = \\mathbb{P}(b_i=c\\mid \\mathbf{a}_i)$ under the softmax model, and $(\\mathbf{p}_i - \\mathbf{e}_{b_i})\\mathbf{a}_i^\\top$ denotes the outer product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be written as\n",
        "$$\n",
        " \\nabla^2 f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I},\\tag{2}\n",
        "$$\n",
        "where $\\otimes$ is the Kronecker product, and $\\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top$ is the softmax Jacobian, which is positive semidefinite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(i)__ (1 point) Is it possible for a strongly convex function to not attain its minimum? Justify your reasoning (you may assume the domain is $\\mathbb{R}^{p\\times C}$).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now show that $f_\\mu$ is smooth, i.e., $\\nabla f_\\mu$ is L-Lipschitz with respect to the Frobenius norm, with a simple conservative bound\n",
        "$$\n",
        " L = \\|\\mathbf{A}\\|_F^2 + \\mu.\n",
        "$$\n",
        "where\n",
        "$$\n",
        " \\mathbf{A} = \\begin{bmatrix}\n",
        "  \\leftarrow &  \\mathbf{a}_1^\\top & \\rightarrow \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_2^\\top & \\rightarrow \\\\\n",
        "   &  \\ldots &  \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_n^\\top & \\rightarrow \\\\\n",
        " \\end{bmatrix}.\n",
        "$$\n",
        "(You may use that the operator norm of the softmax Jacobian is bounded by 1/4, and a looser bound $\\le 1$ is acceptable for grading.)\n",
        "\n",
        "Hint: check the properties of the spectral norm with respect to dot product, Kronecker product, and outer product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 point for all three questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-2)__ Using (2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|\\mathbf{A}\\|_F^2 + \\mu$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(l)__ (1 point) KL divergence and NLL. Let $q(b_i\\mid\\mathbf{a}_i)$ be the true label distribution and $p(b_i\\mid\\mathbf{a}_i)$ the model softmax. Write the KL divergence $\\mathrm{KL}(q\\,\\|\\,p)$ and show that minimizing the KL divergence between $q$ and $p$ is equivalent to minimizing the negative log-likelihood derived in (a).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From your work in this section, you have shown that the maximum likelihood estimator for multiclass softmax logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_F^2$ regularizer. Consequently, the estimator for $\\mathbf{X}$ we will use will be the solution of the smooth strongly convex problem,\n",
        "$$\n",
        " \\mathbf{X}^\\star = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X}) + \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2.\\tag{3}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binary logistic regression (specialization for Part 2)\n",
        "\n",
        "While this part analyzed the multiclass (softmax) setting, in the next exercise we will continue under the simplified two-class case.\n",
        "\n",
        "Let labels be $b_i \\in \\{-1, +1\\}$, features $\\mathbf{a}_i \\in \\mathbb{R}^p$, and weight vector $\\mathbf{x} \\in \\mathbb{R}^p$. Define the sigmoid\n",
        "$$\n",
        "\\sigma(t) = \\frac{1}{1+e^{-t}}.\n",
        "$$\n",
        "Model the conditional distribution as\n",
        "$$\n",
        "\\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma\\big(j\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big), \\quad j \\in \\{-1,+1\\}.\n",
        "$$\n",
        "The likelihood over i.i.d. samples $\\{(\\mathbf{a}_i, b_i)\\}_{i=1}^n$ is\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\prod_{i=1}^n \\sigma\\big(b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big),\n",
        "$$\n",
        "so the negative log-likelihood is\n",
        "$$\n",
        " f(\\mathbf{x}) = -\\log \\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^n \\log\\big(1 + e^{-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
        "$$\n",
        "\n",
        "__(m)__ (1 point) Show that the gradient of the negative log-likelihood is the standard binary logistic regression gradient:\n",
        "$$\n",
        "\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\big(-b_i\\, \\sigma(-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x})\\big)\\, \\mathbf{a}_i.\n",
        "$$\n",
        "(Hint: use the chain rule and $\\sigma'(t) = \\sigma(t)\\big(1-\\sigma(t)\\big)$.)\n",
        "\n",
        "We will use this binary formulation in Part 2 - First order methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "md-hw",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
