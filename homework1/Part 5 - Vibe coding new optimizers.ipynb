{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Vibe-Coding Emerging Optimizers on CIFAR Classification \n",
    "\n",
    "As large language models (LLMs) reshape software engineering, we are entering a new era of AI-assisted programming where human creativity and machine execution merge. Instead of focusing on every low-level implementation detail, developers can prioritize **ideas, exploration, and iteration speed**, while the model handles much of the routine coding. This is the essence of [**vibe coding**](https://en.wikipedia.org/wiki/Vibe_coding?utm_source=chatgpt.com): coding by “vibes” and letting AI accelerate the path from concept to working system. In this homework, you will practice vibe coding by using AI assistants such as [ChatGPT](https://chat.openai.com), [Gemini](https://gemini.google.com/app), or [Claude](https://www.anthropic.com/claude-code) to help design and implement optimizers for CIFAR image classification.  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is *Vibe Coding*? \n",
    "\n",
    "**Vibe coding** is a paradigm popularized by Andrej Karpathy in 2025. It emphasizes generating most of the code through LLMs while the human programmer acts as a **guide, tester, and refiner**. Instead of carefully reviewing every line, you iterate based on execution results, keeping the creative process at the center.  \n",
    "\n",
    "Karpathy put it simply:  \n",
    "> *“Fully giving in to the vibes, embracing exponentials, and forgetting that the code even exists.”*  \n",
    "\n",
    "**Learn more :**\n",
    "- [IBM: What is Vibe Coding?](https://www.ibm.com/think/topics/vibe-coding?utm_source=chatgpt.com)  \n",
    "- [Replit Blog: What is Vibe Coding?](https://blog.replit.com/what-is-vibe-coding?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Task Overview \n",
    "\n",
    "You will implement and compare four optimizers on **CIFAR-10 classification** with two architectures: a **Transformer**  and a **ResNet** .  \n",
    "\n",
    "- **Optimizers**: Muon , Scion , Dion , Adam (baseline)  \n",
    "- **Models**: Transformer, ResNet  \n",
    "- **Comparison metrics**: convergence speed, final test accuracy , training stability  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Optimizers to Implement \n",
    "\n",
    "### (1) Muon \n",
    "Muon applies **Newton–Schulz orthonormalization** to gradient updates of 2D weight matrices, making them invariant to input conditioning.  \n",
    "References:  \n",
    "- [Muon Blog (Keller Jordan)](https://kellerjordan.github.io/posts/muon/?utm_source=chatgpt.com)  \n",
    "- [Deriving Muon (Jeremy Bernstein)](https://jeremybernste.in/writing/deriving-muon?utm_source=chatgpt.com)  \n",
    "- [Muon GitHub Repo](https://github.com/KellerJordan/Muon?utm_source=chatgpt.com)  \n",
    "- [Convergence Bound (arXiv)](https://arxiv.org/abs/2507.01598?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "### (2) Scion \n",
    "Scion constrains updates differently for hidden vs input/output layers, using **spectral norm** for hidden layers and **ℓ∞ norm** for others. This improves stability and hyperparameter transfer.  \n",
    "References:  \n",
    "- [Scion Paper](https://arxiv.org/abs/2502.07529)  \n",
    "- [Scion Official Code](https://github.com/LIONS-EPFL/scion)  \n",
    "\n",
    "---\n",
    "\n",
    "### (3) Dion \n",
    "Dion extends Muon-like orthonormal updates to **distributed training**. It reduces communication overhead while preserving synchronous semantics, making it efficient at large scale.  \n",
    "References:  \n",
    "- [Microsoft Research Blog](https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/?utm_source=chatgpt.com)  \n",
    "- [Dion Paper (arXiv)](https://arxiv.org/html/2504.05295v1?utm_source=chatgpt.com)  \n",
    "- [Dion GitHub Repo](https://github.com/microsoft/dion?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "### (4) Adam \n",
    "Adam is the standard baseline optimizer combining momentum and adaptive learning rates. Use either `Adam` or `AdamW` from PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Steps & Deliverables \n",
    "\n",
    "1. **Model Implementation (20 pts)**  \n",
    "   - Build a Transformer for CIFAR classification.  \n",
    "   - Build a ResNet (ResNet-18 or similar).  \n",
    "\n",
    "2. **Optimizer Integration (30 pts)**  \n",
    "   - Implement Muon, Scion, Dion optimizers using your AI assistant in a form which is compatible with `torch.optimizer`.  \n",
    "   - Use Adam as baseline.  \n",
    "\n",
    "3. **Training & Evaluation (30 pts)**  \n",
    "   - Train both models with all optimizers.  \n",
    "   - Collect metrics: training loss, validation accuracy, time-to-accuracy.  \n",
    "   - Present results with plots and a summary table.  \n",
    "\n",
    "4. **Discussion & Reflection (20 pts)**  \n",
    "   - Compare optimizers in terms of convergence speed, stability, and accuracy.  \n",
    "   - Reflect on your experience using **vibe coding** with AI assistants.  \n",
    "   - What worked well? What challenges did you face?  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Objectives \n",
    "\n",
    "- Understand and implement **novel optimizers** (Muon, Scion, Dion).  \n",
    "- Practice **vibe coding** as a workflow with LLMs.  \n",
    "- Compare optimizer performance on **CIFAR-10** across Transformer and ResNet architectures.  \n",
    "- Analyze results critically and reflect on the coding process.  \n",
    "\n",
    "---\n",
    "\n",
    "Good luck and enjoy vibe-coding your way through optimizers!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Coding starts here......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch, embed_dim) for nn.MultiheadAttention\n",
    "        x2 = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x2, x2, x2)\n",
    "        x = x + attn_out\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
    "                 embed_dim=128, depth=6, num_heads=8, mlp_ratio=4., dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, 1 + n_patches, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.transpose(0, 1)  # Transformer expects (seq_len, batch, embed_dim)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.transpose(0, 1)  # back to (batch, seq_len, embed_dim)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]  # (B, embed_dim)\n",
    "        out = self.head(cls_out)  # (B, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10, in_chans=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=3, stride=1, padding=1, bias=False)  # CIFAR-10 uses 3x3, stride 1 here\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # No maxpool since CIFAR-10 image size is small\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "    \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, planes, stride, downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # No maxpool layer\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet18_cifar():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients')\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(p.data, alpha=group['weight_decay'])\n",
    "                \n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                denom = (exp_avg_sq.sqrt() / (bias_correction2 ** 0.5)).add_(group['eps'])\n",
    "                step_size = group['lr'] / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_schulz_orthonormalize(matrix, num_iters=2):\n",
    "    I = torch.eye(matrix.size(0), device=matrix.device)\n",
    "    Y = matrix\n",
    "    Z = torch.eye(matrix.size(0), device=matrix.device)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        T = 0.5 * (3.0 * I - Z @ Y)\n",
    "        Y = Y @ T\n",
    "        Z = T @ Z\n",
    "    return Y\n",
    "\n",
    "class MuonOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=0, num_orth_iters=5):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, num_orth_iters=num_orth_iters)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "            num_orth_iters = group['num_orth_iters']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    grad = grad + weight_decay * p.data\n",
    "                \n",
    "                # Apply Newton-Schulz only if grad is square 2D matrix\n",
    "                if grad.dim() == 2 and grad.size(0) == grad.size(1):\n",
    "                    ortho_grad = newton_schulz_orthonormalize(grad, num_orth_iters)\n",
    "                else:\n",
    "                    ortho_grad = grad\n",
    "                \n",
    "                p.data.add_(-lr * ortho_grad)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_norm(matrix, power_iterations=1):\n",
    "    u = torch.randn(matrix.size(0), device=matrix.device)\n",
    "    for _ in range(power_iterations):\n",
    "        v = torch.mv(matrix.t(), u)\n",
    "        v = v / (v.norm() + 1e-12)\n",
    "        u = torch.mv(matrix, v)\n",
    "        u = u / (u.norm() + 1e-12)\n",
    "    sigma = u @ matrix @ v\n",
    "    return sigma\n",
    "\n",
    "class ScionOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=0, power_iters=1):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, power_iters=power_iters)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "            power_iters = group['power_iters']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    grad = grad + weight_decay * p.data\n",
    "                \n",
    "                if grad.dim() == 2:\n",
    "                    sigma = spectral_norm(p.data, power_iters)\n",
    "                    grad = grad / (sigma + 1e-6)\n",
    "                elif grad.dim() <= 1:\n",
    "                    inf_norm = grad.abs().max()\n",
    "                    grad = grad / (inf_norm + 1e-6)\n",
    "                \n",
    "                p.data.add_(-lr * grad)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthonormalize(matrix):\n",
    "    WTW = matrix.t() @ matrix\n",
    "    try:\n",
    "        eigvals, eigvecs = torch.linalg.eigh(WTW)\n",
    "        inv_sqrt = eigvecs @ torch.diag(eigvals.clamp_min(1e-6).pow(-0.5)) @ eigvecs.t()\n",
    "        ortho = matrix @ inv_sqrt\n",
    "        return ortho\n",
    "    except RuntimeError:\n",
    "        return matrix\n",
    "\n",
    "class DionOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=0):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    grad = grad + weight_decay * p.data\n",
    "                \n",
    "                if grad.dim() == 2 and grad.size(0) == grad.size(1):\n",
    "                    grad = orthonormalize(grad)\n",
    "                \n",
    "                p.data.add_(-lr * grad)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_cifar10_loaders(batch_size=128, num_workers=8):\n",
    "    # Training augmentations: random crop, horizontal flip, normalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                             std=(0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # Test data: just normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                             std=(0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, device, epochs=20, save_path=None):\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['epoch_times'].append(elapsed)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "        # Save checkpoint if accuracy improves\n",
    "        if save_path and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with val accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = len(history['train_loss'])\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), history['train_loss'], label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss over epochs')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(range(1, epochs+1), history['val_acc'], label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy over epochs')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer with Muon optimizer...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# Assume implementations from before are imported:\n",
    "# SimpleTransformer, resnet18_cifar\n",
    "# MuonOptimizer, ScionOptimizer, DionOptimizer, AdamOptimizer\n",
    "# get_cifar10_loaders, train_and_evaluate, plot_training_history\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size)\n",
    "\n",
    "models = {\n",
    "    \"Transformer\": SimpleTransformer(),\n",
    "    \"ResNet\": resnet18_cifar(),\n",
    "}\n",
    "\n",
    "optimizer_classes = {\n",
    "    \"Muon\": MuonOptimizer,\n",
    "    \"Scion\": ScionOptimizer,\n",
    "    \"Dion\": DionOptimizer,\n",
    "    \"Adam\": AdamOptimizer,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    results[model_name] = {}\n",
    "    for opt_name, OptimizerClass in optimizer_classes.items():\n",
    "        print(f\"Training {model_name} with {opt_name} optimizer...\")\n",
    "        \n",
    "        # Fresh copy of model for each optimizer\n",
    "        current_model = model.__class__() if model_name == \"Transformer\" else resnet18_cifar()\n",
    "        current_model.to(device)\n",
    "        \n",
    "        # Instantiate optimizer\n",
    "        if opt_name == \"Adam\":\n",
    "            optimizer = OptimizerClass(current_model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = OptimizerClass(current_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        save_path = f\"{model_name}_{opt_name}_best.pth\"\n",
    "        \n",
    "        history = train_and_evaluate(\n",
    "            current_model, train_loader, test_loader,\n",
    "            optimizer, criterion, device,\n",
    "            epochs=epochs, save_path=save_path\n",
    "        )\n",
    "        \n",
    "        results[model_name][opt_name] = history\n",
    "\n",
    "# After training, you can use plot_training_history(results['Transformer']['Muon']) etc to plot\n",
    "\n",
    "# This setup runs all experiments sequentially and saves results for later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch312)",
   "language": "python",
   "name": "torch312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
